{"cells":[{"cell_type":"markdown","metadata":{"id":"IBVUaGGazg5F"},"source":["# 🌞 Objectives\n","\n","- [upstage/SOLAR-10.7B-v1.0 ](https://huggingface.co/upstage/SOLAR-10.7B-v1.0)을 사례로 Transform 기반의 LLM 을 [kyujinpy/Open-platypus-Commercial](https://huggingface.co/datasets/kyujinpy/Open-platypus-Commercial) 으로 Intruction Tuning 해보기\n","\n","- Transformer, LoRA 의 간략한 이해해보기\n","\n","- Instruction Tuning 의 간략한 이해해보기\n","\n","- Note\n","  ```\n","  1) Google Colab 을 Google Drive 를 마운트한 환경 기준으로 정리함\n","  2) Google Colab 에서 A100 40GB 필요 (Pro 이상 계정 구입 필요, 10$/month 수준)\n","  ```"]},{"cell_type":"markdown","metadata":{"id":"Ba3qL8As7t51"},"source":["# ⚾ pre-requistes\n","\n","- [Google Colab](https://colab.research.google.com/) 와 A100 40GB 인스턴스  \n","\n","- [Google Drive](https://drive.google.com/drive/) 와 30GB 정도 스토리지 용량\n","\n","  - Google Colab 의 가상머신에 마운트 (자동으로 아래 마운트 될 것)\n","\n","    ```\n","    /content/drive/MyDrive\n","    ```\n","- Hugging Face 계정\n","\n","- `/content/drive/MyDrive/llm` 경로의 폴더 생성\n","\n","- 파이선 패키지 설치\n","\n","  - `huggingface_hub` 의 버전 충돌 발생시 `pip install --upgrade huggingface_hub` 등으로 업데이트\n","\n","  ```bash\n","  pip install bitsandbytes==0.41.1\n","  pip install accelerate==0.21.0\n","  pip install appdirs\n","  pip install loralib\n","  pip install datasets\n","  pip install fire\n","  pip install git+https://github.com/huggingface/peft\n","  pip install transformers==4.34.1\n","  pip install sentencepiece sentence_transformers\n","  pip install scipy numpy scikit-learn pandas\n","  ```\n","- `/content/drive/MyDrive/llm` 에 Hugging Face 에서 모델과 데이터 다운로드 (아래 SSH 공개키를 Hugging Face 에 등록하는 과정 필요할 수 있음)\n","\n","  ```bash\n","  git clone git@hf.co:upstage/SOLAR-10.7B-v1.0\n","  git clone git@hf.co:datasets/kyujinpy/Open-platypus-Commercial\n","  ```\n","\n","  - Google Colab 의 가상머신에서 생성한 공개키를 Hugging Face 의 SSH Key 로 등록\n","\n","    - Colab Console 에서 공개키 생성하여 SSH Agent 에 키 등록\n","\n","      ```bash\n","      ssh-keygen -t ed25519 -C \"your_email@example.com\n","      eval \"$(ssh-agent -s)\n","      ssh-add ~/.ssh/id_ed25519\n","      ```\n","    \n","    - Huggig Face 의 Setting > SSH and GPH Key 메뉴에서 SSH Key 등록\n","      \n","      - 등록할 SSH Key 출력\n","      \n","        ```bash\n","        cat ~/.ssh/id_ed25519.pub\n","        ```"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":79153,"status":"ok","timestamp":1705459234697,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"},"user_tz":-540},"id":"d4VUn9Q1lVo8","outputId":"fe0f69c9-f041-484a-a801-06ee9b3d4bff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting bitsandbytes==0.41.1\n","  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bitsandbytes\n","Successfully installed bitsandbytes-0.41.1\n","Collecting accelerate==0.21.0\n","  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (2.1.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.21.0) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.21.0\n","Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (1.4.4)\n","Collecting loralib\n","  Downloading loralib-0.1.2-py3-none-any.whl (10 kB)\n","Installing collected packages: loralib\n","Successfully installed loralib-0.1.2\n","Collecting transformers==4.34.1\n","  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (0.20.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers==4.34.1)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.1) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.1) (4.5.0)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.34.1)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.1) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.1) (2023.11.17)\n","Installing collected packages: huggingface-hub, tokenizers, transformers\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.20.2\n","    Uninstalling huggingface-hub-0.20.2:\n","      Successfully uninstalled huggingface-hub-0.20.2\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.15.0\n","    Uninstalling tokenizers-0.15.0:\n","      Successfully uninstalled tokenizers-0.15.0\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.35.2\n","    Uninstalling transformers-4.35.2:\n","      Successfully uninstalled transformers-4.35.2\n","Successfully installed huggingface-hub-0.17.3 tokenizers-0.14.1 transformers-4.34.1\n","Collecting datasets==2.1\n","  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.4/325.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (1.23.5)\n","Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (10.0.1)\n","Collecting dill (from datasets==2.1)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (3.4.1)\n","Collecting multiprocess (from datasets==2.1)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (3.9.1)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (0.17.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (23.2)\n","Collecting responses<0.19 (from datasets==2.1)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.1) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.1) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.1) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.1) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.1) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.1) (4.0.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.1) (3.13.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.1) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.1) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.1) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.1) (2023.11.17)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.1) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.1) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.1) (1.16.0)\n","Installing collected packages: dill, responses, multiprocess, datasets\n","Successfully installed datasets-2.1.0 dill-0.3.7 multiprocess-0.70.15 responses-0.18.0\n","Collecting fire\n","  Downloading fire-0.5.0.tar.gz (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire) (1.16.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire) (2.4.0)\n","Building wheels for collected packages: fire\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=c363079bfacd6630d25fcd2bbe4e26088392a2750950fa68a72f7a7c9873af3b\n","  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n","Successfully built fire\n","Installing collected packages: fire\n","Successfully installed fire-0.5.0\n","Collecting git+https://github.com/huggingface/peft\n","  Cloning https://github.com/huggingface/peft to /tmp/pip-req-build-740i6d3r\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-req-build-740i6d3r\n","  Resolved https://github.com/huggingface/peft to commit bf54136a79cc85b0e4c3915b4e1eb158f43c4b73\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (6.0.1)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (2.1.0+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (4.34.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (4.66.1)\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (0.21.0)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (0.4.1)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (0.17.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.13.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2.31.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.2.dev0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.1.3)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.2.dev0) (2.1.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.7.2.dev0) (2023.6.3)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.7.2.dev0) (0.14.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.7.2.dev0) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.7.2.dev0) (1.3.0)\n","Building wheels for collected packages: peft\n","  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for peft: filename=peft-0.7.2.dev0-py3-none-any.whl size=182704 sha256=287f163c32d39ecf52ca41a8e758e875de1885e48a0dbe159b1c160244f7a830\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-2wu0sfm2/wheels/4c/16/67/1002a2d4daa822eff130e6d85b90051b75d2ce0d26b9448e4a\n","Successfully built peft\n","Installing collected packages: peft\n","Successfully installed peft-0.7.2.dev0\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentence_transformers\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.34.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.0+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n","Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.17.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.13.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.3)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.14.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n","Building wheels for collected packages: sentence_transformers\n","  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=a7a38392563100c86151e9b092c8045828054f00da8248a4316188a2936f8b45\n","  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n","Successfully built sentence_transformers\n","Installing collected packages: sentencepiece, sentence_transformers\n","Successfully installed sentence_transformers-2.2.2 sentencepiece-0.1.99\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"]}],"source":["!pip install bitsandbytes==0.41.1\n","!pip install accelerate==0.21.0\n","!pip install appdirs\n","!pip install loralib\n","!pip install transformers==4.34.1\n","!pip install datasets==2.1\n","!pip install fire\n","!pip install git+https://github.com/huggingface/peft\n","!pip install sentencepiece sentence_transformers\n","!pip install scipy numpy scikit-learn pandas"]},{"cell_type":"markdown","metadata":{"id":"SL7sgGI30oK4"},"source":["## ⛳ (1) 파이선 설치 모듈 확인 및 임포트"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"xCNI1AiV0r8i","executionInfo":{"status":"ok","timestamp":1705459245943,"user_tz":-540,"elapsed":11252,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"}}},"outputs":[],"source":["import os\n","import os.path as osp\n","import sys\n","import fire\n","import json\n","from typing import List, Union\n","\n","import torch\n","from torch.nn import functional as F\n","\n","import transformers\n","from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n","from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n","from transformers import LlamaForCausalLM, LlamaTokenizer\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","from datasets import load_dataset\n","\n","from peft import (\n","    LoraConfig,\n","    get_peft_model,\n","    prepare_model_for_int8_training,\n","    set_peft_model_state_dict\n",")\n","from peft import PeftModel"]},{"cell_type":"markdown","metadata":{"id":"nNIhVQM7CSWS"},"source":["## ⛳ (2) Pre-trained 모델 가지고와서 설정하기\n","- SOLAR-10.7B 는 AutoRegressive 방식인 Llama2 구조를 확장한 것임\n","- 모델 가중치는 8bit 로 초기 로드 (Quantization 과정 포함)\n","- 계산 과정에서는 float16 형식을 사용"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":68,"referenced_widgets":["a3302711cfb0447f9dd0ef6611c86537","d00a0391f3b14f6caaa94fa42531580a","48e55502f27b47c4a210cea9c8ba2915","2d227e0ea5094b56a1eea789f1859e1c","43acc618d01c41e0abb0dd7194c6917c","f1dfd6729ec04abf8066ec75f24a277f","d1f93caa397e464eb545067d487a4762","fecb499f6c0e42848c2eb98f0a279b11","855919e7ce87419babd20bbe27836e38","5a24a83e9a124e66893bb757e479a309","e232ae15783844cd96b538eebc5c337b"]},"executionInfo":{"elapsed":196235,"status":"ok","timestamp":1705459442168,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"},"user_tz":-540},"id":"RWisCVD3CBDJ","outputId":"249793a3-031e-458d-dbd6-baf3541c096b"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3302711cfb0447f9dd0ef6611c86537"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[Info] Model is downloaded & loaded !!!\n"]}],"source":["device = 'auto'\n","# base_LLM_model = 'upstage/SOLAR-10.7B-v1.0'\n","base_LLM_model = '/content/drive/MyDrive/llm/SOLAR-10.7B-v1.0'\n","\n","pre_model = AutoModelForCausalLM.from_pretrained(   # SOLAR-10.7B 는 CasualModeling, AR 방식\n","    base_LLM_model,                             # 모델 위치 (hugging face 로그인 사전에 해두어야 함)\n","    load_in_8bit=True,                          # LoRA 를 위해서 8 비트로 로딩\n","    torch_dtype=torch.float16,                  # 연산에 사용할 데이터는 float16\n","    device_map=device)                          # GPU, CPU 등은 알아서 선택\n","\n","print('[Info] Model is downloaded & loaded !!!')"]},{"cell_type":"markdown","metadata":{"id":"_vOhU2KfJA4J"},"source":["\n","##⛳ (3) 토크나이저 정보 확인 및 설정\n","\n","- PAD 는 오른쪽에 채우고 이 토큰을 편의상 0 으로 설정\n","\n","- vocabulary 는 단어와 인덱스 형태로 구성되어 있고, 토크나이저가  문장을 단어 단위로 인덱싱하는데 사용\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1622,"status":"ok","timestamp":1705459443597,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"},"user_tz":-540},"id":"LTxAv5SoCjnS","outputId":"1a484486-5962-4f9d-ccf3-f8512ce15fae"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Info] BOS token: 1\n","[Info] EOS token: 2\n","[Info] PAD token: None\n","[Info] vocabulary size: 32000\n"]}],"source":["pre_tokenizer = AutoTokenizer.from_pretrained(base_LLM_model)\n","\n","bos = pre_tokenizer.bos_token_id\n","eos = pre_tokenizer.eos_token_id\n","pad = pre_tokenizer.pad_token_id\n","pre_tokenizer.padding_side = \"right\"\n","print(\"[Info] BOS token:\", bos)\n","print(\"[Info] EOS token:\", eos)\n","print(\"[Info] PAD token:\", pad)\n","\n","if (pad == None) or (pad == eos):\n","    pre_tokenizer.pad_token_id = 0\n","\n","print(\"[Info] vocabulary size:\",pre_tokenizer.vocab_size)"]},{"cell_type":"markdown","metadata":{"id":"AJUL6tz5MiVa"},"source":["### Tokenizer 동작 확인 및 vocabulary 형태 확인\n","\n","- 이 tokenizer 는 `공백`을 `_` 로 대체하는 것을 볼 수 있음\n","\n","- vcabuary 는 단어와 인덱스로 구성되어 있음"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":132,"status":"ok","timestamp":1705459443598,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"},"user_tz":-540},"id":"PM7EfbCpMUFV","outputId":"f0532ce9-2f29-4388-9845-10e834b8ea29"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 613, 837, 264, 4531]"]},"metadata":{},"execution_count":5}],"source":["pre_tokenizer.encode('i am a boy')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":124,"status":"ok","timestamp":1705459443600,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"},"user_tz":-540},"id":"NVEXqdwoMtFS","outputId":"bcfb48ce-5cba-41b8-8a06-8c0c445c9f00"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["('<s>', '▁i', '▁am', '▁boy')"]},"metadata":{},"execution_count":6}],"source":["def find_key(val):\n","  for key, value in pre_tokenizer.vocab.items():\n","      if value == val:\n","          return key\n","  return None\n","\n","find_key(1), find_key(613), find_key(837), find_key(4531)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":114,"status":"ok","timestamp":1705459443600,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"},"user_tz":-540},"id":"O156r24XM9OQ","outputId":"dde15fad-c2c1-42ad-a47e-04e84325a393"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<s> i am boy'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}],"source":["pre_tokenizer.decode([1, 613, 837, 4531])"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":112,"status":"ok","timestamp":1705459443602,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"},"user_tz":-540},"id":"XJ-3ZM7gOinY","outputId":"d2d29c45-d4f5-4bd0-e40e-2ec39d557643"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['validator', '▁gennaio', '模', 'ctor', 'avi'],\n"," [27201, 25540, 29266, 1810, 17101])"]},"metadata":{},"execution_count":8}],"source":["list(pre_tokenizer.vocab.keys())[0:5] , list(pre_tokenizer.vocab.values())[0:5]"]},{"cell_type":"markdown","metadata":{"id":"3oaNhEEXQsAT"},"source":["## ⛳ (4) Instruct Tuning 할 데이터 확보하기\n","\n","- Instruct Tuning 은 주어진 문제를 어떻게 풀어야 할지 Instruction 을 제공하여 Pretrained 모델을 학습시킨다는 개념\n","\n","- 여기서는 Instruct, Input, Response 의 형태로 프롬프트를 구성하고 프롬프트 상단에는 해당 형태가 이렇게 생겼음을 알려주는 문장을 추가해서 학습 세트를 위한 템플릿 만들었음\n","\n","- 또한 템플릿에는 다운로드한 데이터에서 Intruction, Input 을 채워서 학습용 모델 입력 세트를 구성했음"]},{"cell_type":"markdown","metadata":{"id":"3ceCyX70SLMm"},"source":["### 다운로드한 데이터 로딩하고 확인해보기\n","\n","- 학습 데이터는 input, ouput, instruction 이 주요 컬럼이고 input 이 비어있는 (길이가 0인 문자열) 데이터가 존재함\n","\n","- 이것의 의미는 모델에 input 이 들어오면 output 처럼 대답을 해야 하고 ouput 처럼 문장을 만들때는 instruction 을 고려해야 한다는 의미임\n","\n","- 또한 이 데이터는 MATH/PRM-800K, ARB 등 여러 오픈 데이터 등에서 가지고 온 것을 볼 수 있음"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":189,"referenced_widgets":["a9cc1c790673478caf0eda6528609dda","4ba6ca723809442dba638aea9de49431","fde1e16cab51489ea57e5cfb5bf783a0","620f0b416bbf4d76baa8da6ea7e285fa","81b2ca48f3d54dac91f1e042f7c3c36d","8edc84ce79bd497d9d1a9cdae31bcc77","98e2352ccaad46648da61f36ea77b078","02419a9dc7ec4bb493a61cba734d1509","5006f6ec316a43ef8aa1c1780e810422","c2b17f479bd4439e85900ce88470430e","98031540243d4877a241309cc9c4094e","e2ee31e7136d44d695d2e1fbfaf0b34c","0b017e9e54484130b0a593d6e834c5ee","25541853d2864447b5360da2c8c3e897","0a92ca644be443d49e2bddb09b4df17a","9f0023e90a57479ea8a34bee5adaa488","a2c6eeb7e882469880827cfc08412d69","333dc3018ec5445d80729fee35349fc5","4cf9b89002c9490390583bf4db1c9589","c2431fda13cc4ed5b3fdc2eab97166d6","70fd87199bd1489fbdce0cc4b9738150","cf30ac8551e44c69a9b8b863057bdfee","64e9da07c2ad468d9d6db98accbf759e","ae46ce7ea62049c99e317f5b7d302bdd","604e998a89f848ccb8606c996834f8b3","fccaa641ffab47eabadfcbf399b0effd","9ba32080bd9e4702bffabf990c5de341","73fd381a0be2453f84747035f3e24ca9","d44c15b538c64b39b72b251a3b8765fd","7af014fed5f04ae8ba78eded20eced1d","abf905ee17e44ba8b324133e3921789d","fdd8a4192f9a45ce973712107f12cc2c","7c8423b087f44b1888c0a30fb7a8e81d"]},"executionInfo":{"elapsed":11126,"status":"ok","timestamp":1705459454635,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"},"user_tz":-540},"id":"4FCXwPwHOr-i","outputId":"9f375b8b-a218-4955-aa3a-e6c211fe8b61"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.builder:Using custom data configuration Open-platypus-Commercial-012371d21695f2e4\n"]},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset parquet/Open-platypus-Commercial to /root/.cache/huggingface/datasets/parquet/Open-platypus-Commercial-012371d21695f2e4/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9cc1c790673478caf0eda6528609dda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2ee31e7136d44d695d2e1fbfaf0b34c"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/Open-platypus-Commercial-012371d21695f2e4/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64e9da07c2ad468d9d6db98accbf759e"}},"metadata":{}}],"source":["data = load_dataset('/content/drive/MyDrive/llm/Open-platypus-Commercial')"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":379},"executionInfo":{"elapsed":1291,"status":"ok","timestamp":1705459455923,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"},"user_tz":-540},"id":"lZR8jS2oSAwr","outputId":"be5cf054-9d2b-4a7b-ebee-3b2f7d1d52cd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                       input  \\\n","12348  Choose A, B, C or D as your solution.   \n","12349  Choose A, B, C or D as your solution.   \n","12350  Choose A, B, C or D as your solution.   \n","12351  Choose A, B, C or D as your solution.   \n","12352  Choose A, B, C or D as your solution.   \n","\n","                                                  output  \\\n","12348  The correct answer is $\\mathrm{C}$. To answer ...   \n","12349  The correct answer is A. Although choices B, C...   \n","12350  The correct answer is D. \"At present,\" says th...   \n","12351  The correct answer is D. Although the origin o...   \n","12352  The correct answer is $\\mathrm{C}$. This quest...   \n","\n","                                             instruction data_source  \n","12348  It is perhaps too easy to think of mathematica...         ARB  \n","12349  \"I acknowledge Shakespeare to be the world's g...         ARB  \n","12350  The Shawnee people of the Ohio and Cumberland ...         ARB  \n","12351  \\section{Passage $\\mathrm{V}$}\\nIn linguistics...         ARB  \n","12352  \\section{Passage $\\mathrm{V}$}\\nEngineers and ...         ARB  "],"text/html":["\n","  <div id=\"df-a832b8af-7c27-4135-900d-df71d99a28ed\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>input</th>\n","      <th>output</th>\n","      <th>instruction</th>\n","      <th>data_source</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>12348</th>\n","      <td>Choose A, B, C or D as your solution.</td>\n","      <td>The correct answer is $\\mathrm{C}$. To answer ...</td>\n","      <td>It is perhaps too easy to think of mathematica...</td>\n","      <td>ARB</td>\n","    </tr>\n","    <tr>\n","      <th>12349</th>\n","      <td>Choose A, B, C or D as your solution.</td>\n","      <td>The correct answer is A. Although choices B, C...</td>\n","      <td>\"I acknowledge Shakespeare to be the world's g...</td>\n","      <td>ARB</td>\n","    </tr>\n","    <tr>\n","      <th>12350</th>\n","      <td>Choose A, B, C or D as your solution.</td>\n","      <td>The correct answer is D. \"At present,\" says th...</td>\n","      <td>The Shawnee people of the Ohio and Cumberland ...</td>\n","      <td>ARB</td>\n","    </tr>\n","    <tr>\n","      <th>12351</th>\n","      <td>Choose A, B, C or D as your solution.</td>\n","      <td>The correct answer is D. Although the origin o...</td>\n","      <td>\\section{Passage $\\mathrm{V}$}\\nIn linguistics...</td>\n","      <td>ARB</td>\n","    </tr>\n","    <tr>\n","      <th>12352</th>\n","      <td>Choose A, B, C or D as your solution.</td>\n","      <td>The correct answer is $\\mathrm{C}$. This quest...</td>\n","      <td>\\section{Passage $\\mathrm{V}$}\\nEngineers and ...</td>\n","      <td>ARB</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a832b8af-7c27-4135-900d-df71d99a28ed')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a832b8af-7c27-4135-900d-df71d99a28ed button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a832b8af-7c27-4135-900d-df71d99a28ed');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-0c3c9166-c218-45d9-b911-9a5877d84cea\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0c3c9166-c218-45d9-b911-9a5877d84cea')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-0c3c9166-c218-45d9-b911-9a5877d84cea button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":10}],"source":["import pandas as pd\n","df = pd.DataFrame(data['train'])\n","\n","df[df.apply(lambda x: len(x.input) > 0, axis=1)].head()"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":713,"status":"ok","timestamp":1705459456626,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"},"user_tz":-540},"id":"isldnNTRSXgv","outputId":"b4a7e145-b599-4f9d-e829-3c5d8f4e415e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["MATH/PRM-800K      12298\n","airoboros           2605\n","leetcode_ne         1100\n","guanaco              797\n","ARB                  713\n","scibench             616\n","theoremqa            564\n","tigerbot-kaggle      386\n","Name: data_source, dtype: int64"]},"metadata":{},"execution_count":11}],"source":["df.data_source.value_counts()"]},{"cell_type":"markdown","metadata":{"id":"nC5V8819UzuK"},"source":["### Instruct Tuning 에 쓸 템플릿과 템플릿 기반으로 학습용 입력 데이터를 만들기\n","\n","- `instruct_template` 에 input 이 있는 경우와 없는 경우를 고려해서 prompt tempate 구조 정의\n","\n","- 다운로드한 데이터를 가지고 `instruct_template` 기반으로 학습 데이터 구성할 때 쓸 `Prompter` 만들어 놓기\n","\n","- 다운로드한 데이터, Prompter 를 가지고 토크나이징할 때 쓸 함수 만들어 놓기\n","\n","  -\n","\n","- 토크나이징 하기"]},{"cell_type":"markdown","metadata":{"id":"ufqqukinZjNB"},"source":["#### 프롬프트 구조 정의하기"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"fU5b5B3rUYLg","executionInfo":{"status":"ok","timestamp":1705459456627,"user_tz":-540,"elapsed":8,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"}}},"outputs":[],"source":[" instruct_template = {\n","    \"prompt_input\": '''Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n\n","                    ### Instruction:\\n{instruction}\\n\\n\n","                    ### Input:\\n{input}\\n\\n\n","                    ### Response:\\n''',\n","\n","    \"prompt_no_input\": '''Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\n","                    ### Instruction:\\n{instruction}\\n\\n\n","                    ### Response:\\n''',\n","\n","    \"response_split\": \"### Response:\"\n","}"]},{"cell_type":"markdown","metadata":{"id":"mvfSKT4TZoUM"},"source":["#### 프롬프트 구조에 다운로드한 데이터를 집어넣을 유틸 구현하기"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"bi8d_qK-VJIf","executionInfo":{"status":"ok","timestamp":1705459456627,"user_tz":-540,"elapsed":8,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"}}},"outputs":[],"source":["class Prompter(object):\n","    def __init__(self, verbose: bool = False):\n","        self.template = instruct_template\n","\n","    def generate_prompt(\n","        self,\n","        instruction: str,\n","        input: Union[None, str] = None,\n","        label: Union[None, str] = None,\n","    ) -> str:\n","\n","        if input: # input text가 있다면\n","            res = self.template[\"prompt_input\"].format(\n","                instruction=instruction, input=input\n","            )\n","        else:\n","            res = self.template[\"prompt_no_input\"].format(\n","                instruction=instruction\n","            )\n","\n","        if label:\n","            res = f\"{res}{label}\"\n","\n","        return res\n","\n","    def get_response(self, output: str) -> str:\n","        return output.split(self.template[\"response_split\"])[1].strip()\n","\n","prompter = Prompter()"]},{"cell_type":"markdown","metadata":{"id":"AebVSxKcZxeQ"},"source":["#### Pretrained model 과 함께 제공된 tokenizer 를 이용해서 모델 입력을 만드는 함수 구현하기"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"ut6yb1vBVS8v","executionInfo":{"status":"ok","timestamp":1705459456627,"user_tz":-540,"elapsed":8,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"}}},"outputs":[],"source":["cutoff_len = 4096\n","train_on_inputs = False\n","add_eos_token = False\n","\n","# Tokenizing\n","# - pre-trained 모델 로딩할 때 얻어온 tokenizer 를 이용 최대 입력 길이를 cutoff_len 으로 하고 padding 은 넣지 않도록 설정\n","# - EOS 토큰과 이건 신경 안써도 된다는 attension 마스크를 추가함\n","def tokenize(prompt, add_eos_token=True):\n","    # tokernizer 설정해서 가지고 오기 (cutoff_len 은 하이퍼파라미터 이용)\n","    result = pre_tokenizer( prompt, truncation=True, max_length=cutoff_len, padding=False, return_tensors=None,)\n","\n","    # EOS 토큰 삽입해두고 Note 하기\n","    if (\n","        result[\"input_ids\"][-1] != pre_tokenizer.eos_token_id   # EOS 토큰 없고\n","        and len(result[\"input_ids\"]) < cutoff_len           # 길이가 cutoff_len 보다 작고\n","        and add_eos_token                                   # EOS 붙이기로 했으면\n","    ):\n","        # EOS 토큰을 붙이고 이 EOS 토큰에 집중(attention)하라고 1 로 설정\n","        result[\"input_ids\"].append(pre_tokenizer.eos_token_id)\n","        result[\"attention_mask\"].append(1)\n","\n","    # 모델이 맞춰야 하는것이 입력임 (Auto Regressive 방식의 특징)\n","    result[\"labels\"] = result[\"input_ids\"].copy()\n","\n","    return result\n","\n","# 다운로드 받은 데이터를 Prompter 를 이용해서 구성하고\n","# tokenizer 를 이용해 pre-trained 모델과 함께 제공된 tokenizer 이용해서 입력 만듬\n","def generate_and_tokenize_prompt(data_point):\n","    # tokenizing 하기\n","    full_prompt = prompter.generate_prompt(data_point[\"instruction\"], data_point[\"input\"], data_point[\"output\"])\n","    tokenized_full_prompt = tokenize(full_prompt)\n","\n","    # input 이 없는 경우\n","    if not train_on_inputs:\n","        user_prompt = prompter.generate_prompt(data_point[\"instruction\"], data_point[\"input\"])\n","        tokenized_user_prompt = tokenize(user_prompt, add_eos_token=add_eos_token)\n","        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n","\n","        if add_eos_token:\n","            user_prompt_len -= 1\n","\n","        # -100 으로 채워두어 학습할 때 무시하라고 마킹\n","        tokenized_full_prompt[\"labels\"] = [-100] * user_prompt_len + tokenized_full_prompt[\"labels\"][user_prompt_len:]\n","\n","    return tokenized_full_prompt"]},{"cell_type":"markdown","metadata":{"id":"wDzOGkkAZ6W5"},"source":["#### 학습을 위한 모델 입력 만들기"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88,"referenced_widgets":["cd113385171f41fe814212ec16c30ca8","a9ed978dd94d4c59b76fee71977f0a92","71069858645a4bc5a0a5326406e24a9f","b6966f6b39da48cc8a5ccaa51e728d6c","aabadf8d8372463ea38f8eca325aaa55","774c82ca31b844808ef8d24741e39abb","63eb7d0bf2f3497ba7e80404740cac2a","308d60074d73483fa43d51618be1cd07","b4b7e1db695b4a178d2781bac3dd95ed","e7fab4a717d24ba2a7109c6c1efed2d6","e24e8fc3976e4606b6ec9f18125aacc7"]},"executionInfo":{"elapsed":40289,"status":"ok","timestamp":1705459496909,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"},"user_tz":-540},"id":"DjhhoENpWC3e","outputId":"35328383-183d-4a6d-a932-f66582858f15"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.fingerprint:Parameter 'function'=<function generate_and_tokenize_prompt at 0x7d1e440adab0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/19079 [00:00<?, ?ex/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd113385171f41fe814212ec16c30ca8"}},"metadata":{}}],"source":["data_shuffle = data[\"train\"].shuffle()\n","train_data = data_shuffle.map(generate_and_tokenize_prompt)"]},{"cell_type":"markdown","metadata":{"id":"iYGp8AjeWttF"},"source":["#### 학습용 모델 입력 확인해보기"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"executionInfo":{"elapsed":1507,"status":"ok","timestamp":1705459498404,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"},"user_tz":-540},"id":"Z_FgpbylXBMi","outputId":"e238e327-b969-4b88-82ed-ba8f7eeb0914"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  input                                             output  \\\n","0        Note that $p(x) = 2x$ for $x = -3,$ 4, and 5, ...   \n","1        First, I'll simplify the numerator by performi...   \n","2        Let $AB = x$ and $AC = y$. Then we can write t...   \n","3        Some of the consequences of climate change on ...   \n","4        Since $15$ and $6$ have a common factor of $3,...   \n","\n","                                         instruction    data_source  \n","0  Let $p(x)$ be a cubic polynomial such that $p(...  MATH/PRM-800K  \n","1  Express the following as a common fraction in ...  MATH/PRM-800K  \n","2  Triangle $ABC$ is a right triangle with legs $...  MATH/PRM-800K  \n","3  BEGININPUT\\nBEGINCONTEXT\\ndate: August 5, 2021...      airoboros  \n","4                           Simplify $\\frac{15}{6}.$  MATH/PRM-800K  "],"text/html":["\n","  <div id=\"df-dc6e9a3e-fc1e-4eba-a635-0ac82aa774d6\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>input</th>\n","      <th>output</th>\n","      <th>instruction</th>\n","      <th>data_source</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td></td>\n","      <td>Note that $p(x) = 2x$ for $x = -3,$ 4, and 5, ...</td>\n","      <td>Let $p(x)$ be a cubic polynomial such that $p(...</td>\n","      <td>MATH/PRM-800K</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td></td>\n","      <td>First, I'll simplify the numerator by performi...</td>\n","      <td>Express the following as a common fraction in ...</td>\n","      <td>MATH/PRM-800K</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td></td>\n","      <td>Let $AB = x$ and $AC = y$. Then we can write t...</td>\n","      <td>Triangle $ABC$ is a right triangle with legs $...</td>\n","      <td>MATH/PRM-800K</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td></td>\n","      <td>Some of the consequences of climate change on ...</td>\n","      <td>BEGININPUT\\nBEGINCONTEXT\\ndate: August 5, 2021...</td>\n","      <td>airoboros</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td></td>\n","      <td>Since $15$ and $6$ have a common factor of $3,...</td>\n","      <td>Simplify $\\frac{15}{6}.$</td>\n","      <td>MATH/PRM-800K</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc6e9a3e-fc1e-4eba-a635-0ac82aa774d6')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-dc6e9a3e-fc1e-4eba-a635-0ac82aa774d6 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-dc6e9a3e-fc1e-4eba-a635-0ac82aa774d6');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-0d903cf6-29ad-4bb4-91b6-418d037311a8\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0d903cf6-29ad-4bb4-91b6-418d037311a8')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-0d903cf6-29ad-4bb4-91b6-418d037311a8 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":16}],"source":["df_data_shuffle = pd.DataFrame(data_shuffle)\n","df_data_shuffle.head()"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":833},"executionInfo":{"elapsed":13900,"status":"ok","timestamp":1705459512302,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"},"user_tz":-540},"id":"eEOFpJv6WWXa","outputId":"095522c1-2259-481c-da74-341ece6a9854"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  input                                             output  \\\n","0        Note that $p(x) = 2x$ for $x = -3,$ 4, and 5, ...   \n","1        First, I'll simplify the numerator by performi...   \n","2        Let $AB = x$ and $AC = y$. Then we can write t...   \n","3        Some of the consequences of climate change on ...   \n","4        Since $15$ and $6$ have a common factor of $3,...   \n","\n","                                         instruction    data_source  \\\n","0  Let $p(x)$ be a cubic polynomial such that $p(...  MATH/PRM-800K   \n","1  Express the following as a common fraction in ...  MATH/PRM-800K   \n","2  Triangle $ABC$ is a right triangle with legs $...  MATH/PRM-800K   \n","3  BEGININPUT\\nBEGINCONTEXT\\ndate: August 5, 2021...      airoboros   \n","4                           Simplify $\\frac{15}{6}.$  MATH/PRM-800K   \n","\n","                                           input_ids  \\\n","0  [1, 20811, 349, 396, 13126, 369, 13966, 264, 3...   \n","1  [1, 20811, 349, 396, 13126, 369, 13966, 264, 3...   \n","2  [1, 20811, 349, 396, 13126, 369, 13966, 264, 3...   \n","3  [1, 20811, 349, 396, 13126, 369, 13966, 264, 3...   \n","4  [1, 20811, 349, 396, 13126, 369, 13966, 264, 3...   \n","\n","                                      attention_mask  \\\n","0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n","1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n","2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n","3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n","4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n","\n","                                              labels  \n","0  [-100, -100, -100, -100, -100, -100, -100, -10...  \n","1  [-100, -100, -100, -100, -100, -100, -100, -10...  \n","2  [-100, -100, -100, -100, -100, -100, -100, -10...  \n","3  [-100, -100, -100, -100, -100, -100, -100, -10...  \n","4  [-100, -100, -100, -100, -100, -100, -100, -10...  "],"text/html":["\n","  <div id=\"df-b972b5fe-2a0a-446a-9299-53d0ba417beb\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>input</th>\n","      <th>output</th>\n","      <th>instruction</th>\n","      <th>data_source</th>\n","      <th>input_ids</th>\n","      <th>attention_mask</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td></td>\n","      <td>Note that $p(x) = 2x$ for $x = -3,$ 4, and 5, ...</td>\n","      <td>Let $p(x)$ be a cubic polynomial such that $p(...</td>\n","      <td>MATH/PRM-800K</td>\n","      <td>[1, 20811, 349, 396, 13126, 369, 13966, 264, 3...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td></td>\n","      <td>First, I'll simplify the numerator by performi...</td>\n","      <td>Express the following as a common fraction in ...</td>\n","      <td>MATH/PRM-800K</td>\n","      <td>[1, 20811, 349, 396, 13126, 369, 13966, 264, 3...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td></td>\n","      <td>Let $AB = x$ and $AC = y$. Then we can write t...</td>\n","      <td>Triangle $ABC$ is a right triangle with legs $...</td>\n","      <td>MATH/PRM-800K</td>\n","      <td>[1, 20811, 349, 396, 13126, 369, 13966, 264, 3...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td></td>\n","      <td>Some of the consequences of climate change on ...</td>\n","      <td>BEGININPUT\\nBEGINCONTEXT\\ndate: August 5, 2021...</td>\n","      <td>airoboros</td>\n","      <td>[1, 20811, 349, 396, 13126, 369, 13966, 264, 3...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td></td>\n","      <td>Since $15$ and $6$ have a common factor of $3,...</td>\n","      <td>Simplify $\\frac{15}{6}.$</td>\n","      <td>MATH/PRM-800K</td>\n","      <td>[1, 20811, 349, 396, 13126, 369, 13966, 264, 3...</td>\n","      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n","      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b972b5fe-2a0a-446a-9299-53d0ba417beb')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-b972b5fe-2a0a-446a-9299-53d0ba417beb button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-b972b5fe-2a0a-446a-9299-53d0ba417beb');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-7fa7e7ba-13fb-423a-bad8-f8df637b74ae\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7fa7e7ba-13fb-423a-bad8-f8df637b74ae')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-7fa7e7ba-13fb-423a-bad8-f8df637b74ae button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":17}],"source":["df_train_data = pd.DataFrame(train_data)\n","df_train_data.head()"]},{"cell_type":"markdown","metadata":{"id":"xsJCnoEea8ma"},"source":["## ⛳ (5) Instruct Tuning 위한 PEFT 방식을 LoRA로 정의하기\n","\n","- Pretrained 가중치(모델)에 병렬적으로 Light Weight 한 행렬(모델)을 덧붙이는 방식이 LoRA 임.\n","\n","  - 행렬(모델) 은 <Pretrained 모델의 입력 길이 x 사용자가 정의한 크기> 행렬과 <사용자가 정의한 크기> x <Pretrained 모델의 입력 길이 x 사용자가 정의한 크기> 의 두 개 행렬로 구성\n","\n","  - Light Weight 한 수준은 결과 사용자가 정의하는 것임\n","\n","    - 여기서는 lora_r 로 16으로 정의함\n","\n","  - LoRA 로 튜닝할 때에는 입력이 Pretrained 모델과 요 행렬(모델)로 병렬로 주어지고, 출력시에는 두 모델의 출력이 덧셈이 됨. 다만, 이때 스케일링이 되는데 이것도 사용자가 정의하는 것이고 이는 행력 출력에 곱해짐\n","\n","    - 여기서는 lora_alpha 로 16 으로 정의함\n","\n","- 요 행렬을 LoRA Adpater 라고 하고 Pre-trained 모델의 어느 구성 요소에 병렬로 덧붙일 것인가를 결정해야 함\n","\n","  - 여기서는 gate_proj, down_proj, up_proj 모듈 묶음에 적용하는 것으로 함.\n","\n","  - 참고로 Pretrain 의 모델 구조는 아래처럼 출력됨\n","\n","   ```bash\n","     (model): LlamaModel(\n","    (embed_tokens): Embedding(32000, 4096)\n","    (layers): ModuleList(\n","      (0-47): 48 x LlamaDecoderLayer(\n","        (self_attn): LlamaAttention(\n","          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n","          (k_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n","          (v_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n","          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n","          (up_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n","          (down_proj): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n","          (act_fn): SiLUActivation()\n","        )\n","        (input_layernorm): LlamaRMSNorm()\n","        (post_attention_layernorm): LlamaRMSNorm()\n","      )\n","    )\n","    (norm): LlamaRMSNorm()\n","  )\n","  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n","\n","   ```"]},{"cell_type":"markdown","metadata":{"id":"D_hXaQQ6eWqA"},"source":["#### LoRA 적용 방식 설정하기"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"zBRYbAJYXwbJ","executionInfo":{"status":"ok","timestamp":1705459512302,"user_tz":-540,"elapsed":7,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"}}},"outputs":[],"source":["lora_r = 16\n","lora_alpha = 16\n","lora_dropout = 0.05\n","lora_target_modules = [\"gate_proj\", \"down_proj\", \"up_proj\"]\n","config = LoraConfig(\n","    r=lora_r,\n","    lora_alpha=lora_alpha,\n","    target_modules=lora_target_modules,\n","    lora_dropout=lora_dropout,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\")"]},{"cell_type":"markdown","metadata":{"id":"kb-N8hRgeeOx"},"source":["#### Pretrained 모델에 LoRA 적용"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":861,"status":"ok","timestamp":1705459513157,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"},"user_tz":-540},"id":"YysPNsVaebmu","outputId":"bfd2e546-2b56-4968-d7e6-58b2f59df503"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:143: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n","  warnings.warn(\n"]}],"source":["quant_model = prepare_model_for_int8_training(pre_model)\n","peft_model = get_peft_model(quant_model, config)"]},{"cell_type":"markdown","source":["#### Out of Memory 발생으로 메모리 정리하기"],"metadata":{"id":"2Q4m_dgzRkZ7"}},{"cell_type":"code","source":["quant_model = None\n","model = None\n","\n","import gc\n","gc.collect()\n","torch.cuda.empty_cache()"],"metadata":{"id":"n5QMJzCPRq1X","executionInfo":{"status":"ok","timestamp":1705459978951,"user_tz":-540,"elapsed":487,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GXTctROxfDmn"},"source":["## ⛳ (6) 학습 방석 설정하여 학습기 구성하기\n","\n","- 배치 크기, 학습율, 학습율 변동 방식, 웜밍업 방식, 옵티마이저 등 설정하기. 이렇게 쓴다고 하면 그냥 따라 씀\n","\n","- 이런 설정들로 transformer 의 Trainer 만듬\n","\n","- `30` step 마다 체크포인트,  Google Drive 내 특정 위치(`/content/drive/MyDrive/llm/customized`)에 체크포인트 저장 (최대 2개)\n","\n","- torch.fp16 으로 Pretrained 모델 로딩했으니 여기에 맞추었고 다른 것들은 다른것은 잘..."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"tfutwkSZfKOr","executionInfo":{"status":"ok","timestamp":1705459978951,"user_tz":-540,"elapsed":1,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"}}},"outputs":[],"source":["# 주요 하이퍼파라미터\n","batch_size = 16\n","num_epochs = 1\n","micro_batch = 1 # GPU 1개만 쓸거라서 나누지 않음\n","gradient_accumulation_steps = batch_size // micro_batch\n","lr_scheduler = 'cosine'\n","warmup_ratio = 0.06\n","learning_rate = 4e-4\n","optimizer = 'adamw_torch'\n","weight_decay = 0.01\n","max_grad_norm = 1.0\n","\n","output_dir='/content/drive/MyDrive/llm/customized'\n","trainer = transformers.Trainer(\n","        model=peft_model,\n","        train_dataset=train_data,\n","        eval_dataset=None,\n","        args=transformers.TrainingArguments(\n","            per_device_train_batch_size = micro_batch,\n","            gradient_accumulation_steps = gradient_accumulation_steps,\n","            warmup_ratio=warmup_ratio,\n","            num_train_epochs=num_epochs,\n","            learning_rate=learning_rate,\n","            fp16=True,\n","            logging_steps=1,\n","            optim=optimizer,\n","            evaluation_strategy=\"no\",\n","            save_strategy=\"steps\",\n","            max_grad_norm = max_grad_norm,\n","            save_steps = 5,\n","            lr_scheduler_type=lr_scheduler,\n","            output_dir=output_dir,\n","            save_total_limit=2,\n","            load_best_model_at_end=False,\n","            ddp_find_unused_parameters=False,\n","            group_by_length = False\n","        ),\n","        data_collator=transformers.DataCollatorForSeq2Seq(\n","            pre_tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n","        ),\n","    )"]},{"cell_type":"markdown","metadata":{"id":"_7cKju1xkWAs"},"source":["## ⛳ (7) 학습실행하기\n","\n","- 학습 중간에 에러나서 특정 체크포인트부터 학습하도록 함"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":991},"id":"JiDR-oMTgzAp","executionInfo":{"status":"ok","timestamp":1705460815356,"user_tz":-540,"elapsed":836406,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"}},"outputId":"fa4b350d-88b6-4f15-ce06-0c8b3cf2d849"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Info] Restarting from /content/drive/MyDrive/llm/customized/checkpoint-1170/adapter_model.bin\n","[Info] Parameter volume to be trained\n","trainable params: 42,467,328 || all params: 10,773,991,424 || trainable%: 0.39416522928912257\n"]},{"output_type":"stream","name":"stderr","text":["You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1192' max='1192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1192/1192 13:03, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1171</td>\n","      <td>0.423900</td>\n","    </tr>\n","    <tr>\n","      <td>1172</td>\n","      <td>0.259700</td>\n","    </tr>\n","    <tr>\n","      <td>1173</td>\n","      <td>0.355500</td>\n","    </tr>\n","    <tr>\n","      <td>1174</td>\n","      <td>0.324400</td>\n","    </tr>\n","    <tr>\n","      <td>1175</td>\n","      <td>0.353400</td>\n","    </tr>\n","    <tr>\n","      <td>1176</td>\n","      <td>0.359100</td>\n","    </tr>\n","    <tr>\n","      <td>1177</td>\n","      <td>0.443600</td>\n","    </tr>\n","    <tr>\n","      <td>1178</td>\n","      <td>0.397400</td>\n","    </tr>\n","    <tr>\n","      <td>1179</td>\n","      <td>0.548500</td>\n","    </tr>\n","    <tr>\n","      <td>1180</td>\n","      <td>0.456900</td>\n","    </tr>\n","    <tr>\n","      <td>1181</td>\n","      <td>0.424600</td>\n","    </tr>\n","    <tr>\n","      <td>1182</td>\n","      <td>0.421100</td>\n","    </tr>\n","    <tr>\n","      <td>1183</td>\n","      <td>0.446900</td>\n","    </tr>\n","    <tr>\n","      <td>1184</td>\n","      <td>0.401400</td>\n","    </tr>\n","    <tr>\n","      <td>1185</td>\n","      <td>0.577600</td>\n","    </tr>\n","    <tr>\n","      <td>1186</td>\n","      <td>0.358800</td>\n","    </tr>\n","    <tr>\n","      <td>1187</td>\n","      <td>0.704900</td>\n","    </tr>\n","    <tr>\n","      <td>1188</td>\n","      <td>0.494600</td>\n","    </tr>\n","    <tr>\n","      <td>1189</td>\n","      <td>0.405500</td>\n","    </tr>\n","    <tr>\n","      <td>1190</td>\n","      <td>0.273900</td>\n","    </tr>\n","    <tr>\n","      <td>1191</td>\n","      <td>0.464200</td>\n","    </tr>\n","    <tr>\n","      <td>1192</td>\n","      <td>0.395400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=1192, training_loss=0.00779466085986003, metrics={'train_runtime': 821.8167, 'train_samples_per_second': 23.216, 'train_steps_per_second': 1.45, 'total_flos': 5.786190877267722e+17, 'train_loss': 0.00779466085986003, 'epoch': 1.0})"]},"metadata":{},"execution_count":24}],"source":["resume_from_checkpoint = '/content/drive/MyDrive/llm/customized/checkpoint-1170'\n","if resume_from_checkpoint:\n","    checkpoint_name = os.path.join(\n","        resume_from_checkpoint, \"pytorch_model.bin\"\n","    )  # All checkpoint\n","\n","    if not os.path.exists(checkpoint_name):\n","        checkpoint_name = os.path.join(\n","            resume_from_checkpoint, \"adapter_model.bin\"\n","        )\n","        resume_from_checkpoint = (\n","            True\n","        )\n","\n","    if os.path.exists(checkpoint_name):\n","        print(f\"[Info] Restarting from {checkpoint_name}\")\n","        adapters_weights = torch.load(checkpoint_name)\n","        set_peft_model_state_dict(peft_model, adapters_weights)\n","    else:\n","        print(f\"[Error] Checkpoint {checkpoint_name} not found\")\n","\n","peft_model.config.use_cache = False\n","\n","# 학습규모 파악해보기\n","print('[Info] Parameter volume to be trained')\n","peft_model.print_trainable_parameters()\n","\n","# 학습하기\n","peft_model = torch.compile(peft_model)\n","torch.cuda.empty_cache()\n","trainer.train(resume_from_checkpoint=resume_from_checkpoint)"]},{"cell_type":"markdown","metadata":{"id":"BLo33KIrk4CO"},"source":["## ⛳ (8) 학습 완료된 것 저장하기"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"fNak5sknkv7l","executionInfo":{"status":"ok","timestamp":1705460897576,"user_tz":-540,"elapsed":15211,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a8021a6f-fc05-47ec-cb70-a7faf542a098"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["('/content/drive/MyDrive/llm/customized/tokenizer_config.json',\n"," '/content/drive/MyDrive/llm/customized/special_tokens_map.json',\n"," '/content/drive/MyDrive/llm/customized/tokenizer.model',\n"," '/content/drive/MyDrive/llm/customized/added_tokens.json',\n"," '/content/drive/MyDrive/llm/customized/tokenizer.json')"]},"metadata":{},"execution_count":26}],"source":["peft_model.save_pretrained(output_dir)\n","model_path = os.path.join(output_dir, \"pytorch_model.bin\")\n","torch.save({}, model_path)\n","pre_tokenizer.save_pretrained(output_dir)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"Lse-cgXFk-ih","executionInfo":{"status":"ok","timestamp":1705460911399,"user_tz":-540,"elapsed":458,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c4524300-c287-44af-c384-98ba393e29f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["OptimizedModule(\n","  (_orig_mod): PeftModelForCausalLM(\n","    (base_model): LoraModel(\n","      (model): LlamaForCausalLM(\n","        (model): LlamaModel(\n","          (embed_tokens): Embedding(32000, 4096)\n","          (layers): ModuleList(\n","            (0-47): 48 x LlamaDecoderLayer(\n","              (self_attn): LlamaAttention(\n","                (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n","                (k_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n","                (v_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n","                (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n","                (rotary_emb): LlamaRotaryEmbedding()\n","              )\n","              (mlp): LlamaMLP(\n","                (gate_proj): lora.Linear8bitLt(\n","                  (base_layer): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (up_proj): lora.Linear8bitLt(\n","                  (base_layer): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=4096, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=14336, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (down_proj): lora.Linear8bitLt(\n","                  (base_layer): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n","                  (lora_dropout): ModuleDict(\n","                    (default): Dropout(p=0.05, inplace=False)\n","                  )\n","                  (lora_A): ModuleDict(\n","                    (default): Linear(in_features=14336, out_features=16, bias=False)\n","                  )\n","                  (lora_B): ModuleDict(\n","                    (default): Linear(in_features=16, out_features=4096, bias=False)\n","                  )\n","                  (lora_embedding_A): ParameterDict()\n","                  (lora_embedding_B): ParameterDict()\n","                )\n","                (act_fn): SiLUActivation()\n","              )\n","              (input_layernorm): LlamaRMSNorm()\n","              (post_attention_layernorm): LlamaRMSNorm()\n","            )\n","          )\n","          (norm): LlamaRMSNorm()\n","        )\n","        (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n","      )\n","    )\n","  )\n",")\n"]}],"source":["print(peft_model)"]},{"cell_type":"markdown","source":["## ⛳ (9) 학습 완료된 것을 Pretrained 모델에 통합하기"],"metadata":{"id":"6CGQ2SARaKGK"}},{"cell_type":"markdown","source":["##### Pretrained 베이스 모델과 Lora 수행한 모델 합치기\n","- 학습 완료 직후 여러가지 gc 를 해줘도 메모리 점유율 잘 않떨어짐. 인스턴스 새로 받아서 하는 것이 좋음\n"],"metadata":{"id":"BZI97x4ye0tD"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":77985,"status":"ok","timestamp":1705463498509,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"},"user_tz":-540},"outputId":"f3db0cc5-db73-43ed-d22f-c7bdded962db","id":"RxCV-nC1iK9V"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting bitsandbytes==0.41.1\n","  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bitsandbytes\n","Successfully installed bitsandbytes-0.41.1\n","Collecting accelerate==0.21.0\n","  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (2.1.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.21.0) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.21.0\n","Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (1.4.4)\n","Collecting loralib\n","  Downloading loralib-0.1.2-py3-none-any.whl (10 kB)\n","Installing collected packages: loralib\n","Successfully installed loralib-0.1.2\n","Collecting transformers==4.34.1\n","  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (0.20.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers==4.34.1)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.1) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.1) (4.5.0)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.34.1)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.1) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.1) (2023.11.17)\n","Installing collected packages: huggingface-hub, tokenizers, transformers\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.20.2\n","    Uninstalling huggingface-hub-0.20.2:\n","      Successfully uninstalled huggingface-hub-0.20.2\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.15.0\n","    Uninstalling tokenizers-0.15.0:\n","      Successfully uninstalled tokenizers-0.15.0\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.35.2\n","    Uninstalling transformers-4.35.2:\n","      Successfully uninstalled transformers-4.35.2\n","Successfully installed huggingface-hub-0.17.3 tokenizers-0.14.1 transformers-4.34.1\n","Collecting datasets==2.1\n","  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.4/325.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (1.23.5)\n","Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (10.0.1)\n","Collecting dill (from datasets==2.1)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (3.4.1)\n","Collecting multiprocess (from datasets==2.1)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (3.9.1)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (0.17.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (23.2)\n","Collecting responses<0.19 (from datasets==2.1)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.1) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.1) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.1) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.1) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.1) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.1) (4.0.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.1) (3.13.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.1) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.1) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.1) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.1) (2023.11.17)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.1) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.1) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.1) (1.16.0)\n","Installing collected packages: dill, responses, multiprocess, datasets\n","Successfully installed datasets-2.1.0 dill-0.3.7 multiprocess-0.70.15 responses-0.18.0\n","Collecting fire\n","  Downloading fire-0.5.0.tar.gz (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire) (1.16.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire) (2.4.0)\n","Building wheels for collected packages: fire\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=efebaa31561fd2bbf03aa54bb43369b68417d40a5decc18f2e8a8bf9fc660752\n","  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n","Successfully built fire\n","Installing collected packages: fire\n","Successfully installed fire-0.5.0\n","Collecting git+https://github.com/huggingface/peft\n","  Cloning https://github.com/huggingface/peft to /tmp/pip-req-build-aoxphuob\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-req-build-aoxphuob\n","  Resolved https://github.com/huggingface/peft to commit bf54136a79cc85b0e4c3915b4e1eb158f43c4b73\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (6.0.1)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (2.1.0+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (4.34.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (4.66.1)\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (0.21.0)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (0.4.1)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (0.17.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.13.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2.31.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.2.dev0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.1.3)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.2.dev0) (2.1.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.7.2.dev0) (2023.6.3)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.7.2.dev0) (0.14.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.7.2.dev0) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.7.2.dev0) (1.3.0)\n","Building wheels for collected packages: peft\n","  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for peft: filename=peft-0.7.2.dev0-py3-none-any.whl size=182704 sha256=a0a974a95cfb402cd6ee8645017edef7caaf7783ee54f3f2ee2e8a82c3a3a9f7\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-2c_qivm6/wheels/4c/16/67/1002a2d4daa822eff130e6d85b90051b75d2ce0d26b9448e4a\n","Successfully built peft\n","Installing collected packages: peft\n","Successfully installed peft-0.7.2.dev0\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentence_transformers\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.34.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.0+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n","Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.17.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.13.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.3)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.14.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n","Building wheels for collected packages: sentence_transformers\n","  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=19f3a186930fc6ad3dd8d5c7e5c09f4d0df4ed495eae6f2362455f5731bc1717\n","  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n","Successfully built sentence_transformers\n","Installing collected packages: sentencepiece, sentence_transformers\n","Successfully installed sentence_transformers-2.2.2 sentencepiece-0.1.99\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"]}],"source":["!pip install bitsandbytes==0.41.1\n","!pip install accelerate==0.21.0\n","!pip install appdirs\n","!pip install loralib\n","!pip install transformers==4.34.1\n","!pip install datasets==2.1\n","!pip install fire\n","!pip install git+https://github.com/huggingface/peft\n","!pip install sentencepiece sentence_transformers\n","!pip install scipy numpy scikit-learn pandas"]},{"cell_type":"code","source":["import os\n","import os.path as osp\n","import sys\n","import fire\n","import json\n","from typing import List, Union\n","\n","import torch\n","from torch.nn import functional as F\n","\n","import transformers\n","from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n","from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n","from transformers import LlamaForCausalLM, LlamaTokenizer\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","from datasets import load_dataset\n","\n","from peft import (\n","    LoraConfig,\n","    get_peft_model,\n","    prepare_model_for_int8_training,\n","    set_peft_model_state_dict\n",")\n","from peft import PeftModel\n","\n","# 인스턴스 새로 만들어서 하는 것이 좋음. gc 해줘도 메모리 부족함\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    '/content/drive/MyDrive/llm/SOLAR-10.7B-v1.0',\n","    return_dict = True,\n","    torch_dtype=torch.float16,\n","    device_map='auto')\n"],"metadata":{"id":"MByimrxbaQ5o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pre_tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/llm/SOLAR-10.7B-v1.0')"],"metadata":{"id":"ap3siSkknTgt","executionInfo":{"status":"ok","timestamp":1705464658141,"user_tz":-540,"elapsed":801,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["peft_model = PeftModel.from_pretrained(base_model, '/content/drive/MyDrive/llm/customized', 'auto')\n","fin_model = peft_model.merge_and_unload()"],"metadata":{"id":"78iZaN7jlq0V","executionInfo":{"status":"ok","timestamp":1705464238570,"user_tz":-540,"elapsed":12627,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["#### 합친거 파일로 저장하기"],"metadata":{"id":"gzkbAmxne8F-"}},{"cell_type":"code","source":["final_save_folder = '/content/drive/MyDrive/llm/custom_final'\n","fin_model.save_pretrained(final_save_folder)\n","model_path = os.path.join(final_save_folder, \"pytorch_model.bin\")\n","torch.save({}, model_path)\n","pre_tokenizer.save_pretrained(final_save_folder)"],"metadata":{"id":"38_6gJ9Bevae"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# (10) 최종 Instruction Tuning 모델 동작 확인하기\n","\n","* 메모리 부족할 수 있으니 새로 인스턴스 만들어서 다시 모델 로딩부터 → 40GB 로는 로라랑 베이스모델 못 불러옴\n","\n","* Run Pod 로 옮김"],"metadata":{"id":"LxfLFYR4oW5p"}},{"cell_type":"code","source":["!pip install bitsandbytes==0.41.1\n","!pip install accelerate==0.21.0\n","!pip install appdirs\n","!pip install loralib\n","!pip install transformers==4.34.1\n","!pip install datasets==2.1\n","!pip install fire\n","!pip install git+https://github.com/huggingface/peft\n","!pip install sentencepiece sentence_transformers\n","!pip install scipy numpy scikit-learn pandas"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pCpk4qOMq2m9","executionInfo":{"status":"ok","timestamp":1705470893785,"user_tz":-540,"elapsed":59230,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"}},"outputId":"9f0378a3-f2f6-4f3f-b816-3e5e473e6dc6"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: bitsandbytes==0.41.1 in /usr/local/lib/python3.10/dist-packages (0.41.1)\n","\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: accelerate==0.21.0 in /usr/local/lib/python3.10/dist-packages (0.21.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (2.1.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.21.0) (1.3.0)\n","\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (1.4.4)\n","\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: loralib in /usr/local/lib/python3.10/dist-packages (0.1.2)\n","\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: transformers==4.34.1 in /usr/local/lib/python3.10/dist-packages (4.34.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (0.17.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (2.31.0)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (0.14.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.1) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.1) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.1) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.1) (2023.11.17)\n","\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: datasets==2.1 in /usr/local/lib/python3.10/dist-packages (2.1.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (1.23.5)\n","Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (10.0.1)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (0.70.15)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (3.9.1)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (0.17.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (23.2)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets==2.1) (0.18.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.1) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.1) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.1) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.1) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.1) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.1) (4.0.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.1) (3.13.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.1) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==2.1) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.1) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.1) (2023.11.17)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.1) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.1) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.1) (1.16.0)\n","\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (0.5.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire) (1.16.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire) (2.4.0)\n","\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0mCollecting git+https://github.com/huggingface/peft\n","  Cloning https://github.com/huggingface/peft to /tmp/pip-req-build-1hepiu94\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-req-build-1hepiu94\n","  Resolved https://github.com/huggingface/peft to commit bf54136a79cc85b0e4c3915b4e1eb158f43c4b73\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (6.0.1)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (2.1.0+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (4.34.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (4.66.1)\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (0.21.0)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (0.4.1)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (0.17.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.13.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2.31.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.2.dev0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.1.3)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.2.dev0) (2.1.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.7.2.dev0) (2023.6.3)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.7.2.dev0) (0.14.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.7.2.dev0) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.7.2.dev0) (1.3.0)\n","\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n","Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.34.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.0+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n","Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.17.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.13.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.3)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.14.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n","\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip install --upgrade accelerate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HfrVCN4y_rbs","executionInfo":{"status":"ok","timestamp":1705471930134,"user_tz":-540,"elapsed":5338,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"}},"outputId":"b6d680de-2d77-4279-e9b1-6085bb4b37a8"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.26.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","\n","# 모델과 토크나이저 불러오기\n","device = 'cuda:0'\n","custom_model_path = '/content/drive/MyDrive/llm/custom_final'  # 커스터마이즈된 모델 경로\n","\n","# 커스터마이즈된 모델 로드\n","custom_model = AutoModelForCausalLM.from_pretrained(\n","    custom_model_path,\n","    # load_in_8bit=True,  # 이 옵션은 커스터마이즈된 모델에 따라 조정 필요\n","    # torch_dtype=torch.float16,  # 마찬가지로, 모델에 따라 조정 필요\n","    # device_map=device  # GPU, CPU 등은 알아서 선택\n",")\n","tokenizer = AutoTokenizer.from_pretrained(custom_model_path)\n","\n","print('[Info] Customized model is downloaded & loaded !!!')\n","\n","# 모델을 평가 모드로 설정\n","custom_model.eval()\n","\n","# GPU 사용 가능한 경우 GPU에 모델 로드\n","if torch.cuda.is_available():\n","    custom_model = custom_model.cuda()\n","\n","# 입력 텍스트\n","input_text = \"please translate 'i am a boy' into korean\"\n","\n","# 토크나이즈 하고 텐서로 변환\n","inputs = tokenizer(input_text, return_tensors=\"pt\")\n","if torch.cuda.is_available():\n","    inputs = {k: v.cuda() for k, v in inputs.items()}\n","\n","# 모델 실행하여 출력 얻기\n","with torch.no_grad():\n","    outputs = custom_model(**inputs)\n","\n","# 출력 처리 (예: 텍스트로 변환)\n","output_text = tokenizer.decode(outputs.logits[0], skip_special_tokens=True)\n","\n","print(output_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":566},"id":"onKgbgObobPS","executionInfo":{"status":"error","timestamp":1705473188598,"user_tz":-540,"elapsed":166259,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"}},"outputId":"35affcef-6d69-44c5-ceb1-8786e4f0c91f"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /content/drive/MyDrive/llm/custom_final and are newly initialized: ['layers.16.mlp.gate_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.0.input_layernorm.weight', 'layers.9.self_attn.o_proj.weight', 'layers.19.post_attention_layernorm.weight', 'layers.13.post_attention_layernorm.weight', 'lm_head.weight', 'layers.40.post_attention_layernorm.weight', 'layers.15.self_attn.q_proj.weight', 'layers.17.mlp.down_proj.weight', 'layers.29.self_attn.q_proj.weight', 'layers.33.self_attn.v_proj.weight', 'layers.32.input_layernorm.weight', 'layers.37.self_attn.o_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.6.mlp.down_proj.weight', 'layers.27.mlp.up_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.37.input_layernorm.weight', 'layers.2.self_attn.v_proj.weight', 'layers.37.self_attn.q_proj.weight', 'layers.19.mlp.up_proj.weight', 'layers.31.input_layernorm.weight', 'layers.40.mlp.down_proj.weight', 'layers.31.self_attn.k_proj.weight', 'layers.41.self_attn.q_proj.weight', 'layers.23.self_attn.k_proj.weight', 'layers.28.mlp.gate_proj.weight', 'layers.38.self_attn.k_proj.weight', 'layers.30.mlp.gate_proj.weight', 'layers.41.mlp.up_proj.weight', 'layers.18.mlp.up_proj.weight', 'layers.31.post_attention_layernorm.weight', 'layers.14.input_layernorm.weight', 'layers.17.self_attn.k_proj.weight', 'layers.22.mlp.up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.32.mlp.gate_proj.weight', 'layers.45.self_attn.q_proj.weight', 'layers.18.self_attn.o_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.38.input_layernorm.weight', 'layers.25.self_attn.q_proj.weight', 'layers.16.self_attn.k_proj.weight', 'layers.30.input_layernorm.weight', 'layers.26.mlp.up_proj.weight', 'layers.2.self_attn.q_proj.weight', 'layers.27.input_layernorm.weight', 'layers.20.mlp.gate_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.15.mlp.gate_proj.weight', 'layers.21.input_layernorm.weight', 'layers.13.self_attn.q_proj.weight', 'layers.8.self_attn.q_proj.weight', 'layers.18.self_attn.k_proj.weight', 'layers.33.post_attention_layernorm.weight', 'layers.34.self_attn.k_proj.weight', 'layers.8.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.40.mlp.up_proj.weight', 'layers.12.mlp.down_proj.weight', 'layers.42.self_attn.o_proj.weight', 'layers.29.mlp.up_proj.weight', 'layers.12.self_attn.q_proj.weight', 'layers.24.mlp.down_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.39.mlp.up_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.8.mlp.down_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.15.input_layernorm.weight', 'layers.35.self_attn.o_proj.weight', 'layers.19.input_layernorm.weight', 'layers.24.input_layernorm.weight', 'layers.6.self_attn.q_proj.weight', 'layers.15.mlp.down_proj.weight', 'layers.35.input_layernorm.weight', 'layers.23.self_attn.q_proj.weight', 'layers.30.mlp.up_proj.weight', 'layers.46.self_attn.k_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.11.mlp.down_proj.weight', 'layers.17.mlp.up_proj.weight', 'layers.46.input_layernorm.weight', 'layers.16.self_attn.v_proj.weight', 'layers.37.mlp.up_proj.weight', 'layers.28.self_attn.q_proj.weight', 'layers.14.mlp.gate_proj.weight', 'layers.21.mlp.down_proj.weight', 'layers.18.input_layernorm.weight', 'layers.12.self_attn.v_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.18.mlp.down_proj.weight', 'layers.0.mlp.down_proj.weight', 'layers.46.mlp.gate_proj.weight', 'layers.45.post_attention_layernorm.weight', 'layers.45.mlp.gate_proj.weight', 'layers.21.self_attn.v_proj.weight', 'layers.28.mlp.up_proj.weight', 'layers.30.mlp.down_proj.weight', 'layers.24.mlp.gate_proj.weight', 'layers.32.self_attn.o_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.42.input_layernorm.weight', 'layers.23.self_attn.v_proj.weight', 'layers.20.self_attn.o_proj.weight', 'layers.7.self_attn.k_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.0.self_attn.v_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.29.post_attention_layernorm.weight', 'layers.29.self_attn.k_proj.weight', 'layers.22.post_attention_layernorm.weight', 'layers.32.self_attn.v_proj.weight', 'layers.45.self_attn.k_proj.weight', 'layers.39.self_attn.k_proj.weight', 'layers.42.mlp.gate_proj.weight', 'layers.25.mlp.up_proj.weight', 'layers.33.mlp.down_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.21.self_attn.o_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.22.self_attn.q_proj.weight', 'layers.18.mlp.gate_proj.weight', 'layers.47.input_layernorm.weight', 'layers.44.self_attn.k_proj.weight', 'layers.43.mlp.gate_proj.weight', 'layers.3.mlp.down_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.4.self_attn.q_proj.weight', 'layers.25.mlp.down_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.47.mlp.down_proj.weight', 'layers.27.self_attn.v_proj.weight', 'layers.44.mlp.up_proj.weight', 'layers.19.mlp.down_proj.weight', 'layers.28.self_attn.k_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.34.mlp.down_proj.weight', 'layers.44.self_attn.o_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.8.self_attn.k_proj.weight', 'layers.9.input_layernorm.weight', 'layers.40.self_attn.v_proj.weight', 'layers.10.mlp.down_proj.weight', 'layers.38.mlp.up_proj.weight', 'layers.29.self_attn.v_proj.weight', 'layers.24.self_attn.k_proj.weight', 'layers.39.self_attn.o_proj.weight', 'layers.31.mlp.down_proj.weight', 'layers.4.mlp.down_proj.weight', 'layers.30.self_attn.o_proj.weight', 'layers.39.input_layernorm.weight', 'layers.28.input_layernorm.weight', 'layers.6.self_attn.o_proj.weight', 'layers.17.self_attn.o_proj.weight', 'layers.31.self_attn.v_proj.weight', 'layers.30.self_attn.k_proj.weight', 'layers.17.input_layernorm.weight', 'layers.34.input_layernorm.weight', 'layers.14.self_attn.o_proj.weight', 'layers.30.self_attn.v_proj.weight', 'layers.44.mlp.down_proj.weight', 'layers.42.mlp.down_proj.weight', 'layers.26.self_attn.q_proj.weight', 'layers.27.mlp.gate_proj.weight', 'layers.28.self_attn.o_proj.weight', 'layers.29.mlp.down_proj.weight', 'layers.20.self_attn.v_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.40.self_attn.o_proj.weight', 'layers.1.mlp.down_proj.weight', 'layers.46.self_attn.v_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.45.input_layernorm.weight', 'layers.16.self_attn.q_proj.weight', 'layers.24.self_attn.o_proj.weight', 'layers.32.self_attn.q_proj.weight', 'layers.1.self_attn.q_proj.weight', 'layers.11.input_layernorm.weight', 'layers.27.post_attention_layernorm.weight', 'layers.6.mlp.gate_proj.weight', 'layers.44.self_attn.q_proj.weight', 'layers.42.self_attn.q_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.1.input_layernorm.weight', 'layers.37.self_attn.k_proj.weight', 'layers.9.mlp.down_proj.weight', 'layers.7.self_attn.q_proj.weight', 'layers.13.mlp.gate_proj.weight', 'layers.20.self_attn.k_proj.weight', 'layers.0.self_attn.q_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.34.post_attention_layernorm.weight', 'layers.36.mlp.up_proj.weight', 'layers.46.mlp.down_proj.weight', 'layers.37.mlp.gate_proj.weight', 'layers.20.self_attn.q_proj.weight', 'layers.22.input_layernorm.weight', 'layers.16.self_attn.o_proj.weight', 'layers.16.mlp.down_proj.weight', 'layers.1.self_attn.k_proj.weight', 'layers.28.post_attention_layernorm.weight', 'layers.5.input_layernorm.weight', 'layers.43.mlp.up_proj.weight', 'layers.22.self_attn.o_proj.weight', 'layers.26.self_attn.k_proj.weight', 'layers.34.mlp.gate_proj.weight', 'layers.7.input_layernorm.weight', 'layers.25.post_attention_layernorm.weight', 'layers.5.mlp.gate_proj.weight', 'layers.26.self_attn.v_proj.weight', 'layers.44.input_layernorm.weight', 'layers.13.mlp.up_proj.weight', 'layers.25.self_attn.o_proj.weight', 'layers.22.mlp.down_proj.weight', 'layers.39.self_attn.q_proj.weight', 'layers.6.input_layernorm.weight', 'layers.12.self_attn.k_proj.weight', 'layers.31.self_attn.q_proj.weight', 'layers.35.mlp.up_proj.weight', 'norm.weight', 'layers.28.self_attn.v_proj.weight', 'layers.13.mlp.down_proj.weight', 'layers.14.self_attn.k_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.5.self_attn.k_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.5.mlp.down_proj.weight', 'layers.42.post_attention_layernorm.weight', 'layers.3.self_attn.k_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.16.input_layernorm.weight', 'layers.23.mlp.down_proj.weight', 'layers.25.self_attn.v_proj.weight', 'layers.38.mlp.down_proj.weight', 'layers.20.post_attention_layernorm.weight', 'layers.5.self_attn.q_proj.weight', 'layers.26.input_layernorm.weight', 'layers.5.mlp.up_proj.weight', 'layers.27.self_attn.k_proj.weight', 'layers.19.mlp.gate_proj.weight', 'layers.21.self_attn.q_proj.weight', 'layers.23.self_attn.o_proj.weight', 'layers.33.mlp.gate_proj.weight', 'layers.41.self_attn.v_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.46.self_attn.o_proj.weight', 'layers.37.post_attention_layernorm.weight', 'layers.37.self_attn.v_proj.weight', 'embed_tokens.weight', 'layers.31.self_attn.o_proj.weight', 'layers.44.self_attn.v_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.36.post_attention_layernorm.weight', 'layers.36.mlp.gate_proj.weight', 'layers.43.self_attn.o_proj.weight', 'layers.34.mlp.up_proj.weight', 'layers.25.mlp.gate_proj.weight', 'layers.39.mlp.gate_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.4.mlp.gate_proj.weight', 'layers.11.self_attn.q_proj.weight', 'layers.14.self_attn.q_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.31.mlp.gate_proj.weight', 'layers.17.self_attn.v_proj.weight', 'layers.13.input_layernorm.weight', 'layers.46.self_attn.q_proj.weight', 'layers.47.mlp.gate_proj.weight', 'layers.3.input_layernorm.weight', 'layers.10.self_attn.o_proj.weight', 'layers.33.mlp.up_proj.weight', 'layers.26.post_attention_layernorm.weight', 'layers.28.mlp.down_proj.weight', 'layers.35.self_attn.q_proj.weight', 'layers.22.self_attn.k_proj.weight', 'layers.38.post_attention_layernorm.weight', 'layers.14.self_attn.v_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.39.mlp.down_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.26.self_attn.o_proj.weight', 'layers.43.post_attention_layernorm.weight', 'layers.36.self_attn.q_proj.weight', 'layers.23.mlp.gate_proj.weight', 'layers.20.mlp.down_proj.weight', 'layers.47.self_attn.q_proj.weight', 'layers.21.post_attention_layernorm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.41.input_layernorm.weight', 'layers.39.self_attn.v_proj.weight', 'layers.24.self_attn.q_proj.weight', 'layers.24.self_attn.v_proj.weight', 'layers.10.self_attn.k_proj.weight', 'layers.16.post_attention_layernorm.weight', 'layers.35.post_attention_layernorm.weight', 'layers.32.self_attn.k_proj.weight', 'layers.41.self_attn.k_proj.weight', 'layers.40.mlp.gate_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.23.post_attention_layernorm.weight', 'layers.19.self_attn.q_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.45.mlp.down_proj.weight', 'layers.24.post_attention_layernorm.weight', 'layers.47.post_attention_layernorm.weight', 'layers.4.post_attention_layernorm.weight', 'layers.42.mlp.up_proj.weight', 'layers.47.self_attn.k_proj.weight', 'layers.19.self_attn.k_proj.weight', 'layers.21.self_attn.k_proj.weight', 'layers.20.input_layernorm.weight', 'layers.41.self_attn.o_proj.weight', 'layers.47.self_attn.o_proj.weight', 'layers.47.self_attn.v_proj.weight', 'layers.43.self_attn.q_proj.weight', 'layers.18.post_attention_layernorm.weight', 'layers.20.mlp.up_proj.weight', 'layers.23.mlp.up_proj.weight', 'layers.34.self_attn.o_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.2.input_layernorm.weight', 'layers.45.mlp.up_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.44.mlp.gate_proj.weight', 'layers.21.mlp.gate_proj.weight', 'layers.12.mlp.gate_proj.weight', 'layers.25.self_attn.k_proj.weight', 'layers.32.mlp.down_proj.weight', 'layers.17.self_attn.q_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.29.input_layernorm.weight', 'layers.46.mlp.up_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.25.input_layernorm.weight', 'layers.17.post_attention_layernorm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.mlp.down_proj.weight', 'layers.27.self_attn.q_proj.weight', 'layers.22.mlp.gate_proj.weight', 'layers.43.self_attn.k_proj.weight', 'layers.43.self_attn.v_proj.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.k_proj.weight', 'layers.38.mlp.gate_proj.weight', 'layers.35.mlp.gate_proj.weight', 'layers.18.self_attn.q_proj.weight', 'layers.23.input_layernorm.weight', 'layers.1.self_attn.v_proj.weight', 'layers.43.input_layernorm.weight', 'layers.29.mlp.gate_proj.weight', 'layers.38.self_attn.o_proj.weight', 'layers.0.self_attn.k_proj.weight', 'layers.43.mlp.down_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.9.self_attn.v_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.30.post_attention_layernorm.weight', 'layers.33.self_attn.o_proj.weight', 'layers.44.post_attention_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.36.mlp.down_proj.weight', 'layers.45.self_attn.v_proj.weight', 'layers.13.self_attn.k_proj.weight', 'layers.35.mlp.down_proj.weight', 'layers.33.self_attn.q_proj.weight', 'layers.32.mlp.up_proj.weight', 'layers.36.input_layernorm.weight', 'layers.2.mlp.gate_proj.weight', 'layers.13.self_attn.v_proj.weight', 'layers.10.input_layernorm.weight', 'layers.8.post_attention_layernorm.weight', 'layers.3.post_attention_layernorm.weight', 'layers.36.self_attn.o_proj.weight', 'layers.40.self_attn.k_proj.weight', 'layers.17.mlp.gate_proj.weight', 'layers.27.self_attn.o_proj.weight', 'layers.45.self_attn.o_proj.weight', 'layers.42.self_attn.v_proj.weight', 'layers.21.mlp.up_proj.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.37.mlp.down_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.46.post_attention_layernorm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.35.self_attn.v_proj.weight', 'layers.16.mlp.up_proj.weight', 'layers.6.self_attn.k_proj.weight', 'layers.33.self_attn.k_proj.weight', 'layers.27.mlp.down_proj.weight', 'layers.22.self_attn.v_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.3.self_attn.q_proj.weight', 'layers.30.self_attn.q_proj.weight', 'layers.38.self_attn.q_proj.weight', 'layers.26.mlp.gate_proj.weight', 'layers.26.mlp.down_proj.weight', 'layers.47.mlp.up_proj.weight', 'layers.19.self_attn.o_proj.weight', 'layers.35.self_attn.k_proj.weight', 'layers.40.self_attn.q_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.15.self_attn.k_proj.weight', 'layers.32.post_attention_layernorm.weight', 'layers.40.input_layernorm.weight', 'layers.7.mlp.up_proj.weight', 'layers.39.post_attention_layernorm.weight', 'layers.36.self_attn.v_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.24.mlp.up_proj.weight', 'layers.41.mlp.gate_proj.weight', 'layers.12.input_layernorm.weight', 'layers.19.self_attn.v_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.41.mlp.down_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.34.self_attn.v_proj.weight', 'layers.18.self_attn.v_proj.weight', 'layers.41.post_attention_layernorm.weight', 'layers.38.self_attn.v_proj.weight', 'layers.36.self_attn.k_proj.weight', 'layers.42.self_attn.k_proj.weight', 'layers.29.self_attn.o_proj.weight', 'layers.31.mlp.up_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.33.input_layernorm.weight', 'layers.3.self_attn.v_proj.weight', 'layers.4.input_layernorm.weight', 'layers.34.self_attn.q_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["[Info] Customized model is downloaded & loaded !!!\n"]},{"output_type":"error","ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacty of 39.56 GiB of which 18.81 MiB is free. Process 15709 has 39.54 GiB memory in use. Of the allocated memory 39.03 GiB is allocated by PyTorch, and 20.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-5284b4fa655a>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# GPU 사용 가능한 경우 GPU에 모델 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mcustom_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# 입력 텍스트\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2241\u001b[0m                       \u001b[0mPyTorch\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0musing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0mconversion\u001b[0m \u001b[0mscripts\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mloading\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPyTorch\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mafterwards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                     - A path or url to a model folder containing a *flax checkpoint file* in *.msgpack* format (e.g,\n\u001b[0;32m-> 2243\u001b[0;31m                       `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax` should be set to\n\u001b[0m\u001b[1;32m   2244\u001b[0m                       \u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2245\u001b[0m                     - `None` if you are both providing the configuration and state dictionary (resp. with keyword\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \"\"\"\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \"\"\"\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacty of 39.56 GiB of which 18.81 MiB is free. Process 15709 has 39.54 GiB memory in use. Of the allocated memory 39.03 GiB is allocated by PyTorch, and 20.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]},{"cell_type":"code","source":["import gc\n","del custom_model\n","del tokenizer\n","gc.collect()\n","torch.cuda.empty_cache()"],"metadata":{"id":"dtLuxs4CCyFv","executionInfo":{"status":"ok","timestamp":1705472895689,"user_tz":-540,"elapsed":3,"user":{"displayName":"Song Changsun","userId":"06388535497302534370"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"z9VFTqMxC4SF"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"mount_file_id":"15Gnnn7V2Le7UIRQ_1xEsLuoEBLQs3jrM","authorship_tag":"ABX9TyM5lW5L8MXWh3HkX2Aq8/X0"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a3302711cfb0447f9dd0ef6611c86537":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d00a0391f3b14f6caaa94fa42531580a","IPY_MODEL_48e55502f27b47c4a210cea9c8ba2915","IPY_MODEL_2d227e0ea5094b56a1eea789f1859e1c"],"layout":"IPY_MODEL_43acc618d01c41e0abb0dd7194c6917c"}},"d00a0391f3b14f6caaa94fa42531580a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1dfd6729ec04abf8066ec75f24a277f","placeholder":"​","style":"IPY_MODEL_d1f93caa397e464eb545067d487a4762","value":"Loading checkpoint shards: 100%"}},"48e55502f27b47c4a210cea9c8ba2915":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fecb499f6c0e42848c2eb98f0a279b11","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_855919e7ce87419babd20bbe27836e38","value":5}},"2d227e0ea5094b56a1eea789f1859e1c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a24a83e9a124e66893bb757e479a309","placeholder":"​","style":"IPY_MODEL_e232ae15783844cd96b538eebc5c337b","value":" 5/5 [03:12&lt;00:00, 34.28s/it]"}},"43acc618d01c41e0abb0dd7194c6917c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1dfd6729ec04abf8066ec75f24a277f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1f93caa397e464eb545067d487a4762":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fecb499f6c0e42848c2eb98f0a279b11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"855919e7ce87419babd20bbe27836e38":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5a24a83e9a124e66893bb757e479a309":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e232ae15783844cd96b538eebc5c337b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a9cc1c790673478caf0eda6528609dda":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4ba6ca723809442dba638aea9de49431","IPY_MODEL_fde1e16cab51489ea57e5cfb5bf783a0","IPY_MODEL_620f0b416bbf4d76baa8da6ea7e285fa"],"layout":"IPY_MODEL_81b2ca48f3d54dac91f1e042f7c3c36d"}},"4ba6ca723809442dba638aea9de49431":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8edc84ce79bd497d9d1a9cdae31bcc77","placeholder":"​","style":"IPY_MODEL_98e2352ccaad46648da61f36ea77b078","value":"Downloading data files: 100%"}},"fde1e16cab51489ea57e5cfb5bf783a0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_02419a9dc7ec4bb493a61cba734d1509","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5006f6ec316a43ef8aa1c1780e810422","value":1}},"620f0b416bbf4d76baa8da6ea7e285fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2b17f479bd4439e85900ce88470430e","placeholder":"​","style":"IPY_MODEL_98031540243d4877a241309cc9c4094e","value":" 1/1 [00:00&lt;00:00, 71.95it/s]"}},"81b2ca48f3d54dac91f1e042f7c3c36d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8edc84ce79bd497d9d1a9cdae31bcc77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98e2352ccaad46648da61f36ea77b078":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"02419a9dc7ec4bb493a61cba734d1509":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5006f6ec316a43ef8aa1c1780e810422":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c2b17f479bd4439e85900ce88470430e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98031540243d4877a241309cc9c4094e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e2ee31e7136d44d695d2e1fbfaf0b34c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0b017e9e54484130b0a593d6e834c5ee","IPY_MODEL_25541853d2864447b5360da2c8c3e897","IPY_MODEL_0a92ca644be443d49e2bddb09b4df17a"],"layout":"IPY_MODEL_9f0023e90a57479ea8a34bee5adaa488"}},"0b017e9e54484130b0a593d6e834c5ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2c6eeb7e882469880827cfc08412d69","placeholder":"​","style":"IPY_MODEL_333dc3018ec5445d80729fee35349fc5","value":"Extracting data files: 100%"}},"25541853d2864447b5360da2c8c3e897":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cf9b89002c9490390583bf4db1c9589","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c2431fda13cc4ed5b3fdc2eab97166d6","value":1}},"0a92ca644be443d49e2bddb09b4df17a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_70fd87199bd1489fbdce0cc4b9738150","placeholder":"​","style":"IPY_MODEL_cf30ac8551e44c69a9b8b863057bdfee","value":" 1/1 [00:00&lt;00:00, 35.74it/s]"}},"9f0023e90a57479ea8a34bee5adaa488":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2c6eeb7e882469880827cfc08412d69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"333dc3018ec5445d80729fee35349fc5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4cf9b89002c9490390583bf4db1c9589":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2431fda13cc4ed5b3fdc2eab97166d6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"70fd87199bd1489fbdce0cc4b9738150":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf30ac8551e44c69a9b8b863057bdfee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64e9da07c2ad468d9d6db98accbf759e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ae46ce7ea62049c99e317f5b7d302bdd","IPY_MODEL_604e998a89f848ccb8606c996834f8b3","IPY_MODEL_fccaa641ffab47eabadfcbf399b0effd"],"layout":"IPY_MODEL_9ba32080bd9e4702bffabf990c5de341"}},"ae46ce7ea62049c99e317f5b7d302bdd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73fd381a0be2453f84747035f3e24ca9","placeholder":"​","style":"IPY_MODEL_d44c15b538c64b39b72b251a3b8765fd","value":"100%"}},"604e998a89f848ccb8606c996834f8b3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7af014fed5f04ae8ba78eded20eced1d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_abf905ee17e44ba8b324133e3921789d","value":1}},"fccaa641ffab47eabadfcbf399b0effd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fdd8a4192f9a45ce973712107f12cc2c","placeholder":"​","style":"IPY_MODEL_7c8423b087f44b1888c0a30fb7a8e81d","value":" 1/1 [00:00&lt;00:00, 67.71it/s]"}},"9ba32080bd9e4702bffabf990c5de341":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73fd381a0be2453f84747035f3e24ca9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d44c15b538c64b39b72b251a3b8765fd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7af014fed5f04ae8ba78eded20eced1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abf905ee17e44ba8b324133e3921789d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fdd8a4192f9a45ce973712107f12cc2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c8423b087f44b1888c0a30fb7a8e81d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd113385171f41fe814212ec16c30ca8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a9ed978dd94d4c59b76fee71977f0a92","IPY_MODEL_71069858645a4bc5a0a5326406e24a9f","IPY_MODEL_b6966f6b39da48cc8a5ccaa51e728d6c"],"layout":"IPY_MODEL_aabadf8d8372463ea38f8eca325aaa55"}},"a9ed978dd94d4c59b76fee71977f0a92":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_774c82ca31b844808ef8d24741e39abb","placeholder":"​","style":"IPY_MODEL_63eb7d0bf2f3497ba7e80404740cac2a","value":"100%"}},"71069858645a4bc5a0a5326406e24a9f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_308d60074d73483fa43d51618be1cd07","max":19079,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b4b7e1db695b4a178d2781bac3dd95ed","value":19079}},"b6966f6b39da48cc8a5ccaa51e728d6c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7fab4a717d24ba2a7109c6c1efed2d6","placeholder":"​","style":"IPY_MODEL_e24e8fc3976e4606b6ec9f18125aacc7","value":" 19079/19079 [00:40&lt;00:00, 368.30ex/s]"}},"aabadf8d8372463ea38f8eca325aaa55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"774c82ca31b844808ef8d24741e39abb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63eb7d0bf2f3497ba7e80404740cac2a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"308d60074d73483fa43d51618be1cd07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4b7e1db695b4a178d2781bac3dd95ed":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e7fab4a717d24ba2a7109c6c1efed2d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e24e8fc3976e4606b6ec9f18125aacc7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}