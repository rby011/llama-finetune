# Practice - finetune generative AI models

References

* [How to Fine-Tune an LLM Part 1: Preparing a Dataset for Instruction Tuning | alpaca\_ft â€“ Weights & Biases (wandb.ai)](https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2#%F0%9F%91%89-continue-to-part-2:-training-our-llm)
* [How to Fine-Tune an LLM Part 2: Instruction Tuning Llama 2 | alpaca\_ft â€“ Weights & Biases (wandb.ai)](https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-2-Instruction-Tuning-Llama-2--Vmlldzo1NjY0MjE1)
* [How to Fine-tune an LLM Part 3: The HuggingFace Trainer | alpaca\_ft â€“ Weights & Biases (wandb.ai)](https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-tune-an-LLM-Part-3-The-HuggingFace-Trainer--Vmlldzo1OTEyNjMy)
* [Home â€“ Weights & Biases (wandb.ai)](https://wandb.ai/home)
* [tcapelle/llm\_recipes: A set of scripts and notebooks on LLM finetunning and dataset creation (github.com)](https://github.com/tcapelle/llm_recipes)
* [Hugging Face â€“ The AI community building the future.](https://huggingface.co/kyujinpy)
* [[2308.10792] Instruction Tuning for Large Language Models: A Survey (arxiv.org)](https://arxiv.org/abs/2308.10792)
* [xiaoya-li/Instruction-Tuning-Survey: Project for the paper entitled \`Instruction Tuning for Large Language Models: A Survey\` (github.com)](https://github.com/xiaoya-li/Instruction-Tuning-Survey)
* https://moon-walker.medium.com/%EB%A6%AC%EB%B7%B0-meta-llama%EC%9D%98-%EC%B9%9C%EC%B2%99-stanford-univ%EC%9D%98-alpaca-ec82d432dc25
* https://lmsys.org/blog/2023-03-30-vicuna/
* https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM?tab=readme-ov-file#how-good-is-the-data

## Instruction Tuning - upstage/SOLAR-10.7B-v1.0 (ğŸš€ï¸ it's my first  try)

Objectives

- pre-trained ëª¨ë¸ì„ ëŒ€ìƒìœ¼ë¡œ ì£¼ì–´ì§„ ë¬¸ì œë¥¼ ì–´ë–»ê²Œ ì ‘ê·¼í•´ì•¼ í• ì§€ì— ëŒ€í•´ instruction ì— ğŸš€ï¸ ì´ë¥¼ finetune
- LoRA ê¸°ë°˜ìœ¼ë¡œ parameter efficient finetune ìœ¼ë¡œ mlp layer (ì¼ë¶€ ëª¨ë“ˆ) ì— adapter ë¥¼ ë¶™ì—¬ì„œ ì§„í–‰

Pretrained Model

- [upstage/SOLAR-10.7B-v1.0 Â· Hugging Face](https://huggingface.co/upstage/SOLAR-10.7B-v1.0)

Train Data

- [kyujinpy/Open-platypus-Commercial Â· Datasets at Hugging Face](https://huggingface.co/datasets/kyujinpy/Open-platypus-Commercial)

Result

- [changsun/changsun Â· Hugging Face](https://huggingface.co/changsun/changsun)

Todo ğŸ˜•

- fine tune ì™„ë£Œëœ ëª¨ë¸ì— pre-trained ëª¨ë¸ ì…ë ¥ì²˜ëŸ¼ ë„£ì—ˆëŠ”ë° ë‹µì´ ì—†ìŒ ğŸ‘

## Instruction Tuning - upstage/SOLAR-10.7B-v.1.0 ( second try )

Objectives

- ì´ë¯¸ ë°›ì•„ë†“ì€ ëª¨ë¸ê°€ì§€ê³  ë‹¤ë¥¸ ë°ì´í„° ì…‹ì„ ê°€ì§€ê³  instruction tuning í•´ë´„
- GPT - 4 ê°€ì§€ê³  ë§Œë“  ë°ì´í„° (e.g., alphaca) ë¥¼ ì‚¬ìš©

Pretrained Model (ìƒë™)

Train Data

- //

Result

- //

Todo

- //
